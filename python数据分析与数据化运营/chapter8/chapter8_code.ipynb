{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.dataivy.cn/book/images/head.png)\n",
    "<table align=\"center\" bgcolor=\"#FFFFFF\" border=\"0px\">\n",
    "   <tr bgcolor=\"#FFFFFF\">\n",
    "      <td><img src=\"http://www.dataivy.cn/book/images/release_date.svg\"></td>\n",
    "      <td><img src=\"http://www.dataivy.cn/book/images/python-3.7-green.svg\"></td>\n",
    "      <td><a href=\"http://www.dataivy.cn/blog/python_book_faq/\"><img src=\"http://www.dataivy.cn/book/images/faq-visit_site-blue.svg\"></a></td>\n",
    "      <td><a href=\"http://www.dataivy.cn/blog/python_book_knows_issues/\"><img src=\"http://www.dataivy.cn/book/images/known_issues.svg\"></a></td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> 第八章&nbsp;&nbsp;内容数据化运营</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、案例-基于潜在狄利克雷分配（LDA）的内容主题挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 时间：2019-01-01\n",
    "- 作者：宋天龙（Tony Song）\n",
    "- 程序开发环境：win7 64位\n",
    "- Python版本：64位 3.7\n",
    "- 依赖库：tarfile、os、jieba、gensim、bs4\n",
    "- 程序输入：article.txt、news_data.tar.gz\n",
    "- 程序输出：打印输出18个主题及新文本的预测主题归属"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词\n",
    "def jieba_cut(text):\n",
    "    '''\n",
    "    将输入的文本句子根据词性标注做分词\n",
    "    :param text: 文本句子，字符串型\n",
    "    :return: 符合规则的分词结果\n",
    "    '''\n",
    "    rule_words = ['z', 'vn', 'v', 't', 'nz', 'nr', 'ns', 'n', 'l', 'i', 'j', 'an','a']\n",
    "    words = pseg.cut(text)\n",
    "    seg_list = [word.word for word in words if word.flag in rule_words]\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "def text_pro(words_list, tfidf_object=None, training=True):\n",
    "    '''\n",
    "    gensim主题建模预处理过程，包含分词类别转字典、生成语料库和TF-IDF转换\n",
    "    :param words_list: 分词列表，列表型\n",
    "    :param tfidf_object: TF-IDF模型对象，该对象在训练阶段生成\n",
    "    :param training: 是否训练阶段，用来针对训练和预测两个阶段做预处理\n",
    "    :return: 如果是训练阶段，返回词典、TF-IDF对象和TF-IDF向量空间数据；如果是预测阶段，返回TF-IDF向量空间数据\n",
    "    '''\n",
    "    # 分词列表转字典\n",
    "    dic = corpora.Dictionary(words_list)  # 将分词列表转换为字典形式\n",
    "    print('{:*^60}'.format('token & word mapping review:'))\n",
    "    for i, w in list(dic.items())[:5]:  # 循环读出字典前5条的每个key和value，对应的是索引值和分词\n",
    "        print('token:%s -- word:%s' % (i, w))\n",
    "    # 生成语料库\n",
    "    corpus = [dic.doc2bow(words) for words in words_list]  # 用于存储语料库的列表\n",
    "    print('{:*^60}'.format('bag of words review:'))\n",
    "    print(corpus[0])\n",
    "    # TF-IDF转换\n",
    "    if training:\n",
    "        tfidf = models.TfidfModel(corpus)  # 建立TF-IDF模型对象\n",
    "        corpus_tfidf = tfidf[corpus]  # 得到TF-IDF向量稀疏矩阵\n",
    "        print('{:*^60}'.format('TF-IDF model review:'))\n",
    "        print(list(corpus_tfidf)[0])  # 打印第一条向量\n",
    "        return dic, corpus_tfidf, tfidf\n",
    "    else:\n",
    "        return tfidf_object[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全角转半角\n",
    "def str_convert(content):\n",
    "    '''\n",
    "    将内容中的全角字符，包含英文字母、数字键、符号等转换为半角字符\n",
    "    :param content: 要转换的字符串内容\n",
    "    :return: 转换后的半角字符串\n",
    "    '''\n",
    "    strs = []\n",
    "    for each_char in content:  # 循环读取每个字符\n",
    "        code_num = ord(each_char)  # 读取字符的ASCII值或Unicode值\n",
    "        if code_num == 12288:  # 全角空格直接转换\n",
    "            code_num = 32\n",
    "        elif 65281 <= code_num <= 65374:  # 全角字符（除空格）根据关系转化\n",
    "            code_num -= 65248\n",
    "        strs.append(chr(code_num))\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析文件内容\n",
    "def data_parse(data):\n",
    "    '''\n",
    "    从原始文件中解析出文本内容数据\n",
    "    :param data: 包含代码的原始内容\n",
    "    :return: 文本中的所有内容，列表型\n",
    "    '''\n",
    "    raw_code = BeautifulSoup(data, 'lxml')  # 建立BeautifulSoup对象\n",
    "    content_code = raw_code.find_all('content')  # 从包含文本的代码块中找到content标签\n",
    "    content_list = [str_convert(each_content.text) for each_content in content_code if len(each_content) > 0]\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据并解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压缩文件\n",
    "if not os.path.exists('./news_data'):  # 如果不存在数据目录，则先解压数据文件\n",
    "    with tarfile.open('news_data.tar.gz') as tar:  # 打开tar.gz压缩包对象\n",
    "        names = tar.getnames()  # 获得压缩包内的每个文件对象的名称\n",
    "        for name in names:  # 循环读出每个文件\n",
    "            tar.extract(name, path='./')  # 将文件解压到指定目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总所有内容\n",
    "all_content = []  # 总列表，用于存储所有文件的文本内容\n",
    "for root, dirs, files in os.walk('./news_data'):  # 分别读取遍历目录下的根目录、子目录和文件列表\n",
    "    for file in files:  # 读取每个文件\n",
    "        file_name = os.path.join(root, file)  # 将目录路径与文件名合并为带有完整路径的文件名\n",
    "        with open(file_name, encoding='utf-8') as f:  # 以只读方式打开文件\n",
    "            data = f.read()  # 读取文件内容\n",
    "        all_content.extend(data_parse(data))  # 从文件内容中获取文本并将结果追加到总列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\system_backup\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get word list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.847 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "print('get word list...')\n",
    "words_list = [list(jieba_cut(each_content)) for each_content in all_content]  # 分词列表，用于存储所有文件的分词结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************token & word mapping review:****************\n",
      "token:0 -- word:仇恨\n",
      "token:1 -- word:侮辱\n",
      "token:2 -- word:侵害\n",
      "token:3 -- word:凶杀\n",
      "token:4 -- word:危害\n",
      "********************bag of words review:********************\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "********************TF-IDF model review:********************\n",
      "[(0, 0.16762633852828174), (1, 0.16660204914253687), (2, 0.1643986382302142), (3, 0.168282481745965), (4, 0.16197667368712637), (5, 0.14602961468426073), (6, 0.16282320045073903), (7, 0.10154448591145282), (8, 0.12365275311464316), (9, 0.12399080729729553), (10, 0.16703117734810868), (11, 0.163124879458702), (12, 0.16844765669812112), (13, 0.16409043499326897), (14, 0.1662290891913951), (15, 0.1685028172752526), (16, 0.332245916102828), (17, 0.16383481532598135), (18, 0.16681622559479037), (19, 0.30849126342177313), (20, 0.1677351934753784), (21, 0.16778969587205647), (22, 0.15736459689355045), (23, 0.15266091940783724), (24, 0.11609101090194619), (25, 0.2636835311342954), (26, 0.14576561774317554), (27, 0.16762633852828174), (28, 0.16751768276692697), (29, 0.1653853043789113), (30, 0.16501988564410103), (31, 0.16833748902827933)]\n",
      "********************topic model review:*********************\n",
      "[(0, '0.003*\"比赛\" + 0.002*\"搜狐\" + 0.002*\"登录\" + 0.002*\"欧洲杯\" + 0.001*\"体育\" + 0.001*\"球员\" + 0.001*\"球队\" + 0.001*\"是\" + 0.001*\"体育讯\" + 0.001*\"图\"'), (1, '0.005*\"散布\" + 0.004*\"民族\" + 0.004*\"稳定\" + 0.004*\"标题\" + 0.002*\"谣言\" + 0.002*\"犯罪\" + 0.002*\"赌博\" + 0.002*\"教唆\" + 0.002*\"封建迷信\" + 0.002*\"凶杀\"'), (2, '0.001*\"小区\" + 0.001*\"编号\" + 0.001*\"是\" + 0.001*\"地震\" + 0.001*\"公司\" + 0.001*\"市场\" + 0.001*\"面积\" + 0.001*\"中国\" + 0.001*\"有\" + 0.001*\"房地产\"')]\n"
     ]
    }
   ],
   "source": [
    "dic, corpus_tfidf, tfidf = text_pro(words_list)  # 训练集的文本预处理\n",
    "num_topics = 3  # 设置主题个数\n",
    "lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=num_topics)  # 通过LDA进行主题建模\n",
    "print('{:*^60}'.format('topic model review:'))\n",
    "print(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新数据集的主题模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************token & word mapping review:****************\n",
      "token:0 -- word:一鸣惊人\n",
      "token:1 -- word:三剑客\n",
      "token:2 -- word:上演\n",
      "token:3 -- word:不败\n",
      "token:4 -- word:专业培训\n",
      "********************bag of words review:********************\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 2), (17, 1), (18, 1), (19, 3), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 3), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 2), (38, 1), (39, 1), (40, 2), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 2), (55, 3), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 2), (71, 1), (72, 1), (73, 4), (74, 1), (75, 1), (76, 1), (77, 7), (78, 5), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 4), (90, 4), (91, 1), (92, 1), (93, 7), (94, 1), (95, 1), (96, 2), (97, 3), (98, 1), (99, 2), (100, 2), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 8), (107, 1), (108, 3), (109, 1), (110, 1), (111, 3), (112, 2), (113, 1), (114, 1), (115, 1), (116, 2), (117, 1), (118, 1), (119, 1), (120, 7), (121, 2), (122, 4), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 17), (130, 1), (131, 4), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 2), (140, 2), (141, 2), (142, 1), (143, 1), (144, 1), (145, 2), (146, 1), (147, 2), (148, 1), (149, 2), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 4), (156, 1), (157, 1), (158, 2), (159, 2), (160, 1), (161, 1), (162, 5), (163, 1), (164, 2), (165, 1), (166, 6), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (176, 2), (177, 1), (178, 1), (179, 3), (180, 1), (181, 1), (182, 1), (183, 3), (184, 1), (185, 1), (186, 2)]\n",
      "**********************topic forecast:***********************\n",
      "[[(0, 0.623244), (1, 0.18719506), (2, 0.18956096)]]\n"
     ]
    }
   ],
   "source": [
    "with open('article.txt', encoding='utf-8') as f:  # 打开新的文本\n",
    "    text_new = f.read()  # 读取文本数据\n",
    "text_content = data_parse(data)  # 解析新的文本\n",
    "words_list_new = jieba_cut(text_new)  # 将文本转换为分词列表\n",
    "corpus_tfidf_new = text_pro([words_list_new], tfidf_object=tfidf, training=False)  # 新文本数据集的预处理\n",
    "corpus_lda_new = lda[corpus_tfidf_new]  # 获取新的分词列表（文档）的主题概率分布\n",
    "print('{:*^60}'.format('topic forecast:'))\n",
    "print(list(corpus_lda_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、案例-基于多项式贝叶斯的增量学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 时间：2019-01-01\n",
    "- 作者：宋天龙（Tony Song）\n",
    "- 程序开发环境：win7 64位\n",
    "- Python版本：64位 3.7\n",
    "- 依赖库：re、tarfile、os、numpy、bs4、sklearn\n",
    "- 程序输入：article.txt、news_data.tar.gz\n",
    "- 程序输出：打印输出新的内容所属的主题信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup  # 用于XML格式化处理\n",
    "from sklearn.feature_extraction.text import HashingVectorizer  # 文本转稀疏矩阵\n",
    "from sklearn.naive_bayes import MultinomialNB  # 贝叶斯分类器\n",
    "from sklearn.metrics import accuracy_score  # 分类评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全角转半角\n",
    "def str_convert(content):\n",
    "    '''\n",
    "    将内容中的全角字符，包含英文字母、数字键、符号等转换为半角字符\n",
    "    :param content: 要转换的字符串内容\n",
    "    :return: 转换后的半角字符串\n",
    "    '''\n",
    "    strs = []\n",
    "    for each_char in content:  # 循环读取每个字符\n",
    "        code_num = ord(each_char)  # 读取字符的ASCII值或Unicode值\n",
    "        if code_num == 12288:  # 全角空格直接转换\n",
    "            code_num = 32\n",
    "        elif 65281 <= code_num <= 65374:  # 全角字符（除空格）根据关系转化\n",
    "            code_num -= 65248\n",
    "        strs.append(chr(code_num))\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析文件内容\n",
    "def data_parse(data):\n",
    "    '''\n",
    "    从原始文件中解析出文本内容和标签数据\n",
    "    :param data: 包含代码的原始内容\n",
    "    :return: 以列表形式返回文本中的所有内容和对应标签\n",
    "    '''\n",
    "    raw_code = BeautifulSoup(data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "    doc_code = raw_code.find_all('doc')  # 从包含文本的代码块中找到doc标签\n",
    "    content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "    label_list = []  # 建立空列表，用来存储每个content对应的label的内容\n",
    "    for each_doc in doc_code:  # 循环读出每个doc标签\n",
    "        if len(each_doc) > 0:  # 如果dco标签的内容不为空\n",
    "            content_code = each_doc.find('content')  # 从包含文本的代码块中找到doc标签\n",
    "            raw_content = content_code.text  # 获取原始内容字符串\n",
    "            convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "            content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "\n",
    "            label_code = each_doc.find('url')  # 从包含文本的代码块中找到url标签\n",
    "            label_content = label_code.text  # 获取url信息\n",
    "            label = re.split('[/|.]', label_content)[2]  # 将URL做分割并提取子域名\n",
    "            label_list.append(label)  # 将子域名加入列表\n",
    "    return content_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉检验\n",
    "def cross_val(model_object, data, label):\n",
    "    '''\n",
    "    通过交叉检验计算每次增量学习后的模型得分\n",
    "    :param model_object: 每次增量学习后的模型对象\n",
    "    :param data: 训练数据集\n",
    "    :param label: 训练数据集对应的标签\n",
    "    :return: 交叉检验得分\n",
    "    '''\n",
    "    predict_label = model_object.predict(data)  # 预测测试集标签\n",
    "    score_tmp = round(float(accuracy_score(label, predict_label)),4)  # 计算预测准确率\n",
    "    return score_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to vector\n",
    "def word_to_vector(data):\n",
    "    '''\n",
    "    将训练集文本数据转换为稀疏矩阵\n",
    "    :param data: 输入的文本列表\n",
    "    :return: 稀疏矩阵\n",
    "    '''\n",
    "    model_vector = HashingVectorizer(alternate_sign=False)  # 建立HashingVectorizer对象\n",
    "    vector_data = model_vector.fit_transform(data)  # 将输入文本转化为稀疏矩阵\n",
    "    return vector_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to vecotr\n",
    "def label_to_vector(label, unique_list):\n",
    "    '''\n",
    "    将文本标签转换为向量标签\n",
    "    :param label: 文本列表\n",
    "    :unique_list: 唯一值列表\n",
    "    :return: 向量标签列表\n",
    "    '''\n",
    "    for each_index, each_data in enumerate(label):  # 循环读取每个标签的索引及对应值\n",
    "        label[each_index] = unique_list.index(each_data)  # 将值替换为其索引\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压缩文件\n",
    "if not os.path.exists('./news_data'):  # 如果不存在数据目录，则先解压数据文件\n",
    "    print('extract data from news_data.tar.gz...')\n",
    "    with tarfile.open('news_data.tar.gz') as tar:  # 打开tar.gz压缩包对象\n",
    "        names = tar.getnames()  # 获得压缩包内的每个文件对象的名称\n",
    "        for name in names:  # 循环读出每个文件\n",
    "            tar.extract(name, path='./')  # 将文件解压到指定目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义对象\n",
    "all_content = []  # 列表，用于存储所有训练集的文本内容\n",
    "all_label = []  # 列表，用于存储所有训练集的标签\n",
    "score_list = []  # 列表，用于存储每次交叉检验得分\n",
    "pre_list = []  # 列表，用于存储每次增量计算后的预测标签\n",
    "unique_list = ['sports', 'house', 'news']  # 标签唯一值列表\n",
    "model_nb = MultinomialNB()  # 建立MultinomialNB模型对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉检验和预测数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉检验集\n",
    "with open('test_sets.txt', encoding='utf-8') as f:\n",
    "    test_data = f.read()\n",
    "test_content, test_label = data_parse(test_data)  # 解析文本内容和标签\n",
    "test_data_vector = word_to_vector(test_content)  # 将文本内容向量化\n",
    "test_label_vecotr = label_to_vector(test_label, unique_list)  # 将标签内容向量化\n",
    "\n",
    "# 预测集\n",
    "with open('article.txt', encoding='utf-8') as f:\n",
    "    new_data = f.read()\n",
    "new_content, new_label = data_parse(new_data)  # 解析文本内容和标签\n",
    "new_data_vector = word_to_vector(new_content)  # 将文本内容向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增量学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************incremental learning...*******************\n",
      "training file: news.sohunews.010806.txt\n",
      "training file: news.sohunews.020806.txt\n",
      "training file: news.sohunews.030806.txt\n",
      "training file: news.sohunews.040806.txt\n",
      "training file: news.sohunews.050806.txt\n",
      "training file: news.sohunews.060806.txt\n",
      "training file: news.sohunews.070806.txt\n",
      "training file: news.sohunews.080806.txt\n",
      "training file: news.sohunews.110806.txt\n",
      "training file: news.sohunews.120806.txt\n"
     ]
    }
   ],
   "source": [
    "print('{:*^60}'.format('incremental learning...'))\n",
    "for root, dirs, files in os.walk('./news_data'):  # 分别读取遍历目录下的根目录、子目录和文件列表\n",
    "    for file in files:  # 读取每个文件\n",
    "        file_name = os.path.join(root, file)  # 将目录路径与文件名合并为带有完整路径的文件名\n",
    "        print('training file: %s' % file)\n",
    "        # 增量训练\n",
    "        with open(file_name, encoding='utf-8') as f:  # 以只读方式打开文件\n",
    "            data = f.read()  # 读取文件内容\n",
    "        content, label = data_parse(data)  # 解析文本内容和标签\n",
    "        data_vector = word_to_vector(content)  # 将文本内容向量化\n",
    "        label_vecotr = label_to_vector(label, unique_list)  # 将标签内容向量化\n",
    "        model_nb.partial_fit(np.abs(data_vector), label_vecotr, classes=np.array([0, 1, 2]))  # 增量学习\n",
    "        # 交叉检验\n",
    "        score_list.append(cross_val(model_nb, test_data_vector, test_label_vecotr))  # 将交叉检验结果存入列表\n",
    "        # 增量预测\n",
    "        predict_y = model_nb.predict(new_data_vector)  # 预测内容标签\n",
    "        pre_list.append(predict_y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************cross validation score:*******************\n",
      "[0.8707, 0.9013, 0.9067, 0.9088, 0.9104, 0.912, 0.9147, 0.9147, 0.9147, 0.9158]\n",
      "*********************predicted labels:**********************\n",
      "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "************************true labels:************************\n",
      "['sports']\n"
     ]
    }
   ],
   "source": [
    "print('{:*^60}'.format('cross validation score:'))\n",
    "print(score_list)  # 打印输出每次交叉检验得分\n",
    "print('{:*^60}'.format('predicted labels:'))\n",
    "print(pre_list)  # 打印输出每次预测标签索引值\n",
    "print('{:*^60}'.format('true labels:'))\n",
    "print(new_label)  # 打印输出正确的标签值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
